## âž— Linear Regression Algorithm

### ðŸ“– Definition  
Linear regression fits a model of the form  
$$y_i = \beta_0 + \beta_1 x_{i1} + \beta_2 x_{i2} + \dots + \beta_p x_{ip} + \varepsilon_i
$$  
by estimating coefficients $\beta = [\beta_0,\dots,\beta_p]$ that **minimize the sum of squared residuals**.

---

### ðŸ”¢ Steps to Apply Linear Regression & Compute Râ€‘squared / RMSE

1. **Data Preparation**  
   - Handle missing values (imputation or removal).  
   - Encode categorical variables (oneâ€‘hot, ordinal).  
   - (Optional) Standardize or normalize features if scales vary.

2. **Fit the Model**  
   - **Closed form** (Normal Equation):  
     $$hat\beta = (X^\top X)^{-1} X^\top y
     $$  
   - **Iterative** (Gradient Descent):  
     Update $beta \leftarrow \beta - \alpha\,\nabla_{\beta} \bigl\|X\beta - y\bigr\|^2$ until convergence.

3. **Make Predictions**  
   - Compute fitted values:  
     $$hat y = X\,\hat\beta.
     $$

4. **Compute Residuals**  
   - For each sample:  
     $$e_i = y_i - \hat y_i.
     $$

5. **Calculate Râ€‘squared**  
   Measures the proportion of variance explained:  
   $$R^2
   = 1 \;-\;
   \frac{\sum_{i=1}^N (y_i - \hat y_i)^2}
        {\sum_{i=1}^N (y_i - \bar y)^2}
   \,,\quad
   \bar y = \frac{1}{N}\sum_{i=1}^N y_i.
   $$

6. **Calculate RMSE**  
   The root mean squared error:  
   $$mathrm{RMSE}
   = \sqrt{\frac{1}{N}\sum_{i=1}^N (y_i - \hat y_i)^2}.
   $$

7. **Evaluate & Diagnose**  
   - **Residual plots**: check for nonâ€‘random patterns (linearity, homoscedasticity).  
   - **QQâ€‘plot**: assess normality of residuals.  
   - **Outliers / Influential points**: identify via Cookâ€™s distance or leverage.  

---

ðŸŽ¯ **Summary**  
Follow these steps to train a linear model, predict, and quantify its performance via $R^2$ (explained variance) and RMSE (average error magnitude).

---
## ðŸ”¢ Statistical Concepts

---

### ðŸ”¹ Difference between Classification and Regression

| Aspect               | Classification                              | Regression                                        |
|----------------------|---------------------------------------------|---------------------------------------------------|
| **Target Variable**  | Discrete categories (classes)               | Continuous numeric values                         |
| **Example Task**     | Spam vs. Ham email prediction               | Predicting house prices                           |
| **Model Output**     | Class labels (e.g. â€œcatâ€, â€œdogâ€)            | Realâ€‘valued estimate (e.g. \$350{,}000\$)         |
| **Typical Loss**     | Crossâ€‘entropy, hinge loss                   | Mean Squared Error (MSE), Mean Absolute Error     |
| **Evaluation**       | Accuracy, precision/recall, F1â€‘score        | \$R^2\$, RMSE, MAE                                |

---

### ðŸ”¹ What Does **Maximum Likelihood Estimation** Mean?

**Maximum Likelihood Estimation (MLE)** is a method for estimating the parameters \(\theta\) of a statistical model. You choose \(\theta\) to **maximize** the probability (likelihood) of observing your data under that model.

1. **Likelihood Function**  
   Given data \(\mathcal{D} = \{x_1, x_2, \dots, x_N\}\) and a parametric model \(p(x \mid \theta)\), define the likelihood:
   $$(\theta)
   = P(\mathcal{D} \mid \theta)
   = \prod_{i=1}^{N} p(x_i \mid \theta).
   $$

2. **Logâ€‘Likelihood**  
   Itâ€™s more convenient to maximize the log of the likelihood:
   $$ell(\theta)
   = \log L(\theta)
   = \sum_{i=1}^{N} \log p(x_i \mid \theta).
   $$

3. **MLE Estimate**  
   The MLE of \(\theta\) is
   $$hat{\theta}
   = \arg\max_{\theta}\,L(\theta)
   = \arg\max_{\theta}\,\ell(\theta).
   $$

4. **Example (Gaussian Mean)**  
   If \$(x_i \sim \mathcal{N}(\mu, \sigma^2)\)$ with known \(\sigma^2\), the logâ€‘likelihood is
   $$ell(\mu)
   = -\frac{N}{2}\log(2\pi\sigma^2)
     -\frac{1}{2\sigma^2}\sum_{i=1}^N (x_i - \mu)^2.
   $$
   Maximizing this w.r.t. \(\mu\) yields
   $$hat{\mu}
   = \frac{1}{N}\sum_{i=1}^N x_i
   $$  
   â€” the sample mean.

---

ðŸŽ¯ **Takeaway**  
MLE finds the parameter values that make the observed data most probable under your chosen model.  

----
## ðŸ”„ Expectationâ€“Maximization (EM) & Its Connection to Maximum Likelihood

### ðŸ“– What Is Expectationâ€“Maximization?

The **Expectationâ€“Maximization (EM)** algorithm is an iterative method for finding **maximum likelihood estimates** of parameters in statistical models that depend on **unobserved (latent) variables**. EM alternates between:

1. **Eâ€‘Step (Expectation)**  
   Compute the expected value of the **completeâ€‘data logâ€‘likelihood**, using the current parameter estimate \(\theta^{(t)}\):  
   $$Q\bigl(\theta \mid \theta^{(t)}\bigr)
   = \mathbb{E}_{Z\mid X,\theta^{(t)}}\bigl[\log p(X,Z\mid \theta)\bigr],
   $$  
   where \(X\) is the observed data and \(Z\) are the latent variables.

2. **Mâ€‘Step (Maximization)**  
   Maximize this expectation to update the parameters:  
   $$theta^{(t+1)}
   = \arg\max_{\theta}\;Q\bigl(\theta \mid \theta^{(t)}\bigr).
   $$

These two steps are repeated until convergence (i.e., until \(\theta^{(t+1)}\approx \theta^{(t)}\)).

---

### ðŸŽ¯ How EM Relates to Maximum Likelihood

- **Objective**: EM seeks to **maximize the observedâ€‘data logâ€‘likelihood**  
  $$ell(\theta)
  = \log p(X \mid \theta)
  = \log \int p(X,Z \mid \theta)\,dZ.
  $$
- **Challenge**: Direct maximization may be intractable when the integral over \(Z\) is complex.
- **EM Solution**: By introducing the auxiliary function \(Q(\theta\mid\theta^{(t)})\), EM guarantees that each iteration **does not decrease** the observedâ€‘data likelihood:
  $$ell\bigl(\theta^{(t+1)}\bigr)
  \;\ge\;
  \ell\bigl(\theta^{(t)}\bigr).
  $$
- In effect, EM finds a sequence of parameter estimates that **monotonically increase** the likelihood, converging to a (local) maximum of the likelihood function.

---

### ðŸ“Œ Example: Gaussian Mixture Model (GMM)

For a GMM with \(K\) components, latent \$(Z_i\in\{1,\dots,K\}\)$:

- **Eâ€‘Step**: Compute responsibilities  
  $$gamma_{ik}
  = P\bigl(Z_i = k \mid x_i,\theta^{(t)}\bigr).
  $$
- **Mâ€‘Step**: Update mixing weights \(\pi_k\), means \$(\mu_k\)$, and covariances \$(\Sigma_k\)$ using \$(\{\gamma_{ik}\}\$).

This iteratively maximizes the mixture modelâ€™s likelihood, which would be hard to optimize directly.

---

> **Bottom Line**:  
> EM is a practical, generalâ€‘purpose algorithm for **MLE** in models with latent variables, turning an intractable optimization into two simpler steps that guarantee nonâ€‘decreasing likelihood.  
